version: '3.8'

networks:
  backend:
    name: backend
    driver: bridge

volumes:
  zookeeper_data:
  kafka_data:
  prometheus_data:

services:
    ### KAFKA ###
    zookeeper:
        image: zookeeper:latest
        container_name: zookeeper
        environment:
          ZOOKEEPER_CLIENT_PORT: 2181
          ZOOKEEPER_TICK_TIME: 2000
          ZOO_4LW_COMMANDS_WHITELIST: ruok
        ports:
          - "2181:2181"
        healthcheck:
            test: ["CMD-SHELL", "[ $(echo ruok | nc 127.0.0.1 2181) = \"imok\" ] || exit 1"]
            interval: 10s
            timeout: 5s
            retries: 5
        networks:
            - backend
        volumes:
            - zookeeper_data:/var/lib/zookeeper
            
    kafka:
        image: wurstmeister/kafka:latest
        container_name: kafka
        depends_on:
            zookeeper:
                condition: service_healthy
        environment:
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: DOCKER_INTERNAL:PLAINTEXT,DOCKER_EXTERNAL:PLAINTEXT
            KAFKA_LISTENERS: DOCKER_INTERNAL://:29092,DOCKER_EXTERNAL://:9092
            KAFKA_ADVERTISED_LISTENERS: DOCKER_INTERNAL://kafka:29092,DOCKER_EXTERNAL://${HOST_IP:-localhost}:9092
            KAFKA_INTER_BROKER_LISTENER_NAME: DOCKER_INTERNAL
            KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
            KAFKA_BROKER_ID: 1
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
            KAFKA_CREATE_TOPICS: "search_terms:1:1, question_base:1:1" 
        ports:
            - "9092:9092"
        healthcheck:
            test: "/bin/sh -c '/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092'"
            interval: 10s
            timeout: 5s
            retries: 5
        networks:
            - backend
        volumes:
            - kafka_data:/var/lib/kafka

    kafdrop:
        image: obsidiandynamics/kafdrop:latest
        container_name: kafdrop
        restart: "no"
        ports:
            - "9000:9000"
        environment:
            KAFKA_BROKERCONNECT: "kafka:29092"
        depends_on:
            - "kafka"
        networks:
            - backend

    kafka-exporter:
        image: danielqsj/kafka-exporter:latest
        environment:
            KAFKA_SERVER: kafka:29092
        command: ["--kafka.server=kafka:29092", "--zookeeper.server=zookeeper:2181"]
        ports:
            - "9308:9308"
        depends_on:
            kafka:
                condition: service_healthy
        networks:
            - backend
            
    kafka-connect:
        image: confluentinc/cp-kafka-connect-base:6.0.0
        container_name: kafka-connect
        platform: linux/amd64
        depends_on:
            - zookeeper
            - kafka
        ports:
            - 8083:8083
        environment:
            CONNECT_BOOTSTRAP_SERVERS: "kafka:29092"
            CONNECT_REST_PORT: 8083
            CONNECT_GROUP_ID: kafka-connect
            CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
            CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
            CONNECT_STATUS_STORAGE_TOPIC: _connect-status
            CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
            CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
            CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
            CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
            CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
            CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
            CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
            CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
            CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
            CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
            CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
            CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components,/data/connect-jars
        volumes:
            - ./kafka-connect-data:/data
        command:
            - /bin/bash
            - -c
            - |
                echo "Installing Connector"
                confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:10.0.1
                #
                echo "Launching Kafka Connect worker"
                /etc/confluent/docker/run &
                #
                sleep infinity
        networks:
            - backend

    setup-kafka-connect:
        container_name: setup-kafka-connect
        image: curlimages/curl:latest
        depends_on:
            - kafka-connect
        entrypoint: >
            /bin/sh -c "
                echo 'Waiting for Kafka Connect to start...';
                sleep 10;
                curl -X POST -H 'Content-Type: application/json' --data '{
                \"name\": \"elasticsearch-sink\",
                \"config\": {
                    \"connector.class\": \"io.confluent.connect.elasticsearch.ElasticsearchSinkConnector\",
                    \"tasks.max\": \"1\",
                    \"type.name\": \"_doc\",
                    \"topics\": \"question_base\",
                    \"key.ignore\": \"true\",
                    \"schema.ignore\": \"true\",
                    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",
                    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",
                    \"value.converter.schemas.enable\": \"false\",
                    \"connection.url\": \"http://elasticsearch:9200\"
                }
                }' http://kafka-connect:8083/connectors;
                "
        networks:
            - backend

    kafka-connect-ui:
        image: landoop/kafka-connect-ui
        restart: always
        depends_on:
            - kafka-connect
        ports:
            - "8000:8000"
        environment:
            CONNECT_URL: http://kafka-connect:8083
        networks:
            - backend
    ### ELK ###
    elasticsearch:
        image: docker.elastic.co/elasticsearch/elasticsearch:7.17.25
        container_name: elasticsearch
        environment:
            ELASTIC_PASSWORD: elastic_password
            network.host: '0.0.0.0'
            xpack.security.enabled: false
            discovery.type: single-node
            ES_JAVA_OPTS: "-Xms512m -Xmx512m"
        ports:
            - "9200:9200"
        networks:
            - backend

    elasticsearch_exporter:
        image: quay.io/prometheuscommunity/elasticsearch-exporter:latest
        command:
        - '--es.uri=http://elasticsearch:9200'
        restart: always
        ports:
        - "9114:9114"
        networks:
            - backend

    kibana:
        image: docker.elastic.co/kibana/kibana:7.17.25
        container_name: kibana
        depends_on:
            - elasticsearch
        environment:
            ELASTICSEARCH_HOSTS: http://elasticsearch:9200
            ELASTICSEARCH_PASSWORD: elastic_password
        ports:
            - "5601:5601"
        networks:
            - backend

    ### Monitoring ###
    prometheus:
        image: prom/prometheus:latest
        container_name: prometheus
        volumes:
            - prometheus_data:/prometheus
            - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
        ports:
            - "9090:9090"
        networks:
            - backend

    grafana:
        image: grafana/grafana:latest
        container_name: grafana
        environment:
            GF_SECURITY_ADMIN_USER: admin
            GF_SECURITY_ADMIN_PASSWORD: admin
            GF_INSTALL_PLUGINS: "hamedkarbasi93-kafka-datasource" 
            GF_DASHBOARDS_JSON_ENABLED: "true" 
        depends_on:
            - prometheus
        ports:
            - "3000:3000"
        healthcheck:
            test: ["CMD-SHELL", "curl -fsSL http://localhost:3000/api/health | grep -q '\"database\":true'"]
            interval: 10s
            timeout: 5s
            retries: 5
        volumes:
            - ./grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
            - ./grafana/dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml
            - ./grafana/dashboards:/var/lib/grafana/dashboards
        networks:
            - backend

    ### Redis ###
    redis:
        container_name: redis
        image: redis/redis-stack:latest
        restart: always
        healthcheck:
            test: ["CMD", "redis-cli","ping"]
            interval: 10s
            timeout: 5s
            retries: 3
        networks:
            - backend
        ports:
        - "6379:6379"

    redis-ui:
        image: redis/redisinsight:latest
        ports:
        - "5540:5540"
        depends_on: 
            redis:
                condition: service_healthy
                restart: true
        networks:
            - backend

    ml-service:
        build: ./ml_service
        container_name: ml-service
        restart: always
        environment:
            EMB_PATH_GLOVE: "./artifacts/glove.6B.50d.txt"
            EMB_PATH_KNRM: "./artifacts/embeddings.bin"
            MLP_PATH: "./artifacts/knrm_mlp.bin"
            VOCAB_PATH: "./artifacts/vocab.json"
        networks:
            - backend
        volumes:
            - ./ml_service/artifacts:/usr/src/app/artifacts