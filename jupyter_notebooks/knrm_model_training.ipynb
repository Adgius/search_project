{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOMAhAjvlvt2"
   },
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip\n",
    "!unzip -qq QQP-clean.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lf9gcXcpoHgC"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -qq glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqVteY6qlx7k",
    "outputId": "3ce2b176-111c-41fb-9710-60742397fb53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "test = pd.read_csv('/content/QQP/test.tsv', sep='\\t')\n",
    "train = pd.read_csv('/content/QQP/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrO6Uky6f6v8"
   },
   "outputs": [],
   "source": [
    "glue_qqp_dir = '/data/QQP/'\n",
    "glove_path = '/data/glove.6B.50d.txt'\n",
    "\n",
    "\n",
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1., sigma: float = 1.):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        transformation = -torch.pow((x - self.mu), 2) / (2 * self.sigma ** 2)\n",
    "        return torch.exp(transformation)\n",
    "\n",
    "\n",
    "class KNRM(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix: np.ndarray, freeze_embeddings: bool, kernel_num: int = 21,\n",
    "                 sigma: float = 0.1, exact_sigma: float = 0.001,\n",
    "                 out_layers: List[int] = [10, 5]):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=freeze_embeddings,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.cosine_sim = torch.nn.CosineSimilarity(dim=3, eps=1e-6)\n",
    "        self.kernel_num = kernel_num\n",
    "        self.sigma = sigma\n",
    "        self.exact_sigma = exact_sigma\n",
    "        self.out_layers = out_layers\n",
    "\n",
    "        self.kernels = self._get_kernels_layers()\n",
    "\n",
    "        self.mlp = self._get_mlp()\n",
    "\n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
    "\n",
    "        def _get_kernels_mu(k):\n",
    "            step = 1 / (k - 1)\n",
    "            left = -1 + step\n",
    "            right = 1 - step\n",
    "            return np.hstack([np.arange(left, right, (right - left)/ (k - 2)), right, 1])\n",
    "\n",
    "        kernels = torch.nn.ModuleList()\n",
    "        mu = _get_kernels_mu(self.kernel_num)\n",
    "        for m in mu:\n",
    "            if m != 1:\n",
    "                kernel = GaussianKernel(m, self.sigma)\n",
    "            else:\n",
    "                kernel = GaussianKernel(1, self.exact_sigma)\n",
    "            kernels.append(kernel)\n",
    "        return kernels\n",
    "\n",
    "    def _get_mlp(self) -> torch.nn.Sequential:\n",
    "        fnn_lst = []\n",
    "\n",
    "        current_nn = self.kernel_num\n",
    "        for l in self.out_layers:\n",
    "            fnn_lst.append(torch.nn.ReLU())\n",
    "            fnn_lst.append(torch.nn.Linear(current_nn, l))\n",
    "            current_nn = l\n",
    "\n",
    "        fnn_lst.append(torch.nn.ReLU())\n",
    "        fnn_lst.append(torch.nn.Linear(current_nn, 1))\n",
    "\n",
    "        return torch.nn.Sequential(*fnn_lst)\n",
    "\n",
    "    def forward(self, input_1: Dict[str, torch.Tensor], input_2: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        logits_1 = self.predict(input_1)\n",
    "        logits_2 = self.predict(input_2)\n",
    "\n",
    "        logits_diff = logits_1 - logits_2\n",
    "\n",
    "        out = self.out_activation(logits_diff)\n",
    "        return out\n",
    "\n",
    "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
    "        alpha = 1e-6\n",
    "        query = self.embeddings(query)\n",
    "        doc = self.embeddings(doc)\n",
    "        nominator = (query.unsqueeze(dim=2) * doc.unsqueeze(dim=1)).sum(axis=3)\n",
    "        denuminator = torch.sqrt(torch.sum(query * query, axis=2) + alpha).unsqueeze(dim=2) * torch.sqrt(torch.sum(doc * doc, axis=2) + alpha).unsqueeze(dim=1)\n",
    "        matrix = nominator / (denuminator)\n",
    "        return matrix\n",
    "\n",
    "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        KM = []\n",
    "        for kernel in self.kernels:\n",
    "            # shape = [B]\n",
    "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "            KM.append(K)\n",
    "\n",
    "        # shape = [B, K]\n",
    "        kernels_out = torch.stack(KM, dim=1)\n",
    "        return kernels_out\n",
    "\n",
    "    def predict(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        # shape = [Batch, Left], [Batch, Right]\n",
    "        query, doc = inputs['query'], inputs['document']\n",
    "\n",
    "        # shape = [Batch, Left, Right]\n",
    "        matching_matrix = self._get_matching_matrix(query, doc)\n",
    "        # shape = [Batch, Kernels]\n",
    "        kernels_out = self._apply_kernels(matching_matrix)\n",
    "        # shape = [Batch]\n",
    "        out = self.mlp(kernels_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, index_pairs_or_triplets: List[List[Union[str, float]]],\n",
    "                 idx_to_text_mapping: Dict[str, str], vocab: Dict[str, int], oov_val: int,\n",
    "                 preproc_func: Callable, max_len: int = 30):\n",
    "        self.index_pairs_or_triplets = index_pairs_or_triplets\n",
    "        self.idx_to_text_mapping = idx_to_text_mapping\n",
    "        self.vocab = vocab\n",
    "        self.oov_val = oov_val\n",
    "        self.preproc_func = preproc_func\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_pairs_or_triplets)\n",
    "\n",
    "    def _tokenized_text_to_index(self, tokenized_text: List[str]) -> List[int]:\n",
    "        return [self.vocab.get(t, self.oov_val) for t in tokenized_text]\n",
    "\n",
    "    def _convert_text_idx_to_token_idxs(self, idx: int) -> List[int]:\n",
    "        text = self.idx_to_text_mapping[idx]\n",
    "        preproc_text = self.preproc_func(text)\n",
    "        return self._tokenized_text_to_index(preproc_text)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TrainTripletsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        # Один запрос сравнивается с двумя документами\n",
    "        query_doc_label = self.index_pairs_or_triplets[idx]\n",
    "        left_elem, right_elem = {}, {}\n",
    "        left_elem['query'] = self._convert_text_idx_to_token_idxs(query_doc_label[0])\n",
    "        left_elem['document'] = self._convert_text_idx_to_token_idxs(query_doc_label[1])\n",
    "        right_elem['query'] = self._convert_text_idx_to_token_idxs(query_doc_label[0])\n",
    "        right_elem['document'] = self._convert_text_idx_to_token_idxs(query_doc_label[2])\n",
    "        label = torch.tensor(query_doc_label[3]).float()\n",
    "        return left_elem, right_elem, label\n",
    "\n",
    "\n",
    "\n",
    "class ValPairsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        query_doc_label = self.index_pairs_or_triplets[idx]\n",
    "        left_elem = {}\n",
    "        left_elem['query'] = self._convert_text_idx_to_token_idxs(query_doc_label[0])\n",
    "        left_elem['document'] = self._convert_text_idx_to_token_idxs(query_doc_label[1])\n",
    "        label = torch.tensor(query_doc_label[2])\n",
    "        return left_elem, label\n",
    "\n",
    "\n",
    "def collate_fn(batch_objs: List[Union[Dict[str, torch.Tensor], torch.FloatTensor]]):\n",
    "    max_len_q1 = -1\n",
    "    max_len_d1 = -1\n",
    "    max_len_q2 = -1\n",
    "    max_len_d2 = -1\n",
    "\n",
    "    is_triplets = False\n",
    "    for elem in batch_objs:\n",
    "        if len(elem) == 3:\n",
    "            left_elem, right_elem, label = elem\n",
    "            is_triplets = True\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        max_len_q1 = max(len(left_elem['query']), max_len_q1)\n",
    "        max_len_d1 = max(len(left_elem['document']), max_len_d1)\n",
    "        if len(elem) == 3:\n",
    "            max_len_q2 = max(len(right_elem['query']), max_len_q2)\n",
    "            max_len_d2 = max(len(right_elem['document']), max_len_d2)\n",
    "\n",
    "    q1s = []\n",
    "    d1s = []\n",
    "    q2s = []\n",
    "    d2s = []\n",
    "    labels = []\n",
    "\n",
    "    for elem in batch_objs:\n",
    "        if is_triplets:\n",
    "            left_elem, right_elem, label = elem\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        pad_len1 = max_len_q1 - len(left_elem['query'])\n",
    "        pad_len2 = max_len_d1 - len(left_elem['document'])\n",
    "        if is_triplets:\n",
    "            pad_len3 = max_len_q2 - len(right_elem['query'])\n",
    "            pad_len4 = max_len_d2 - len(right_elem['document'])\n",
    "\n",
    "        q1s.append(left_elem['query'] + [0] * pad_len1)\n",
    "        d1s.append(left_elem['document'] + [0] * pad_len2)\n",
    "        if is_triplets:\n",
    "            q2s.append(right_elem['query'] + [0] * pad_len3)\n",
    "            d2s.append(right_elem['document'] + [0] * pad_len4)\n",
    "        labels.append([label])\n",
    "    q1s = torch.LongTensor(q1s)\n",
    "    d1s = torch.LongTensor(d1s)\n",
    "    if is_triplets:\n",
    "        q2s = torch.LongTensor(q2s)\n",
    "        d2s = torch.LongTensor(d2s)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    ret_left = {'query': q1s, 'document': d1s}\n",
    "    if is_triplets:\n",
    "        ret_right = {'query': q2s, 'document': d2s}\n",
    "        return ret_left, ret_right, labels\n",
    "    else:\n",
    "        return ret_left, labels\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, glue_qqp_dir: str, glove_vectors_path: str,\n",
    "                 min_token_occurancies: int = 1,\n",
    "                 random_seed: int = 0,\n",
    "                 emb_rand_uni_bound: float = 0.2,\n",
    "                 freeze_knrm_embeddings: bool = True,\n",
    "                 knrm_kernel_num: int = 21,\n",
    "                 knrm_out_mlp: List[int] = [],\n",
    "                 dataloader_bs: int = 1024,\n",
    "                 train_lr: float = 0.001,\n",
    "                 change_train_loader_ep: int = 10\n",
    "                 ):\n",
    "        self.glue_qqp_dir = glue_qqp_dir\n",
    "        self.glove_vectors_path = glove_vectors_path\n",
    "        now = round(time.time())\n",
    "        self.glue_train_df = self.get_glue_df('train')\n",
    "        print(\"self.get_glue_df('train')\", round(time.time()) - now); now = round(time.time())\n",
    "        self.glue_dev_df = self.get_glue_df('dev')\n",
    "        print(\"self.get_glue_df('dev')\", round(time.time()) - now); now = round(time.time())\n",
    "        self.dev_pairs_for_ndcg = self.create_val_pairs(self.glue_dev_df)\n",
    "        print(\"self.create_val_pairs\", round(time.time()) - now); now = round(time.time())\n",
    "        self.min_token_occurancies = min_token_occurancies\n",
    "        self.all_tokens = self.get_all_tokens(\n",
    "            [self.glue_train_df, self.glue_dev_df], self.min_token_occurancies)\n",
    "        print(\"self.get_all_tokens\", round(time.time()) - now); now = round(time.time())\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        self.emb_rand_uni_bound = emb_rand_uni_bound\n",
    "        self.freeze_knrm_embeddings = freeze_knrm_embeddings\n",
    "        self.knrm_kernel_num = knrm_kernel_num\n",
    "        self.knrm_out_mlp = knrm_out_mlp\n",
    "        self.dataloader_bs = dataloader_bs\n",
    "        self.train_lr = train_lr\n",
    "        self.change_train_loader_ep = change_train_loader_ep\n",
    "\n",
    "        self.model, self.vocab, self.unk_words = self.build_knrm_model()\n",
    "        print(\"self.build_knrm_model\", round(time.time()) - now); now = round(time.time())\n",
    "        self.idx_to_text_mapping_train = self.get_idx_to_text_mapping(\n",
    "            self.glue_train_df)\n",
    "        print(\"self.get_idx_to_text_mapping\", round(time.time()) - now); now = round(time.time())\n",
    "        self.idx_to_text_mapping_dev = self.get_idx_to_text_mapping(\n",
    "            self.glue_dev_df)\n",
    "        print(\"self.idx_to_text_mapping_dev\", round(time.time()) - now); now = round(time.time())\n",
    "\n",
    "        self.val_dataset = ValPairsDataset(self.dev_pairs_for_ndcg,\n",
    "              self.idx_to_text_mapping_dev,\n",
    "              vocab=self.vocab, oov_val=self.vocab['OOV'],\n",
    "              preproc_func=self.simple_preproc)\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(\n",
    "            self.val_dataset, batch_size=self.dataloader_bs, num_workers=0,\n",
    "            collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    def get_glue_df(self, partition_type: str) -> pd.DataFrame:\n",
    "        assert partition_type in ['dev', 'train']\n",
    "        glue_df = pd.read_csv(\n",
    "            self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', on_bad_lines='skip', dtype=object)\n",
    "        glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "        glue_df_fin = pd.DataFrame({\n",
    "            'id_left': glue_df['qid1'],\n",
    "            'id_right': glue_df['qid2'],\n",
    "            'text_left': glue_df['question1'],\n",
    "            'text_right': glue_df['question2'],\n",
    "            'label': glue_df['is_duplicate'].astype(int)\n",
    "        })\n",
    "        return glue_df_fin\n",
    "\n",
    "    def handle_punctuation(self, inp_str: str) -> str:\n",
    "        inp_str = re.sub(r\"\"\"[!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\"\"\", ' ', inp_str)\n",
    "        return inp_str\n",
    "\n",
    "    def simple_preproc(self, inp_str: str) -> List[str]:\n",
    "        inp_str = self.handle_punctuation(inp_str)\n",
    "        inp_str = inp_str.lower()\n",
    "        return nltk.word_tokenize(inp_str)\n",
    "\n",
    "    def _filter_rare_words(self, vocab: Dict[str, int], min_occurancies: int) -> Dict[str, int]:\n",
    "        return {k: v for k, v in vocab.items() if v >= min_occurancies}\n",
    "\n",
    "    def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "        all_text = pd.concat(list_of_df, ignore_index=True)\n",
    "        all_text = pd.concat([all_text['text_left'], all_text['text_right']]).drop_duplicates()\n",
    "        token_counts = Counter(token for sentence in all_text for token in self.simple_preproc(sentence))\n",
    "        keys = self._filter_rare_words(token_counts, min_occurancies).keys()\n",
    "        return list(keys)\n",
    "\n",
    "    def _read_glove_embeddings(self, file_path: str) -> Dict[str, List[str]]:\n",
    "        with open(file_path) as f:\n",
    "            emb = f.readlines()\n",
    "            embeddings = {}\n",
    "            for row in emb:\n",
    "                row = row.split()\n",
    "                embeddings.update({row[0]: row[1:]})\n",
    "        return embeddings\n",
    "\n",
    "    def create_glove_emb_from_file(self, file_path: str, inner_keys: List[str],\n",
    "                                   random_seed: int, rand_uni_bound: float\n",
    "                                   ) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
    "        embs = self._read_glove_embeddings(file_path)\n",
    "        print('glove_emb', len(embs))\n",
    "        dim = len(list(embs.values())[0])\n",
    "        pad_token = np.zeros(dim)\n",
    "        oov_token = np.random.uniform(-0.2, 0.2, dim)\n",
    "\n",
    "        words = list(embs.keys())\n",
    "        print('inner_keys', len(words))\n",
    "        unk_words = list(set(inner_keys).difference(set(words)))\n",
    "        inner_keys = ['PAD'] + ['OOV'] + inner_keys\n",
    "        matrix =  np.random.uniform(-rand_uni_bound, rand_uni_bound, (len(inner_keys), dim))\n",
    "        matrix[0] = pad_token\n",
    "        matrix[1] = oov_token\n",
    "        for n, w in enumerate(inner_keys[2:]):\n",
    "            if w in embs:\n",
    "                matrix[n + 2] = embs[w]\n",
    "        matrix = np.array(matrix).astype(float)\n",
    "        word2token = {k: n for n, k in enumerate(inner_keys)}\n",
    "        return matrix, word2token, unk_words\n",
    "\n",
    "    def build_knrm_model(self) -> Tuple[torch.nn.Module, Dict[str, int], List[str]]:\n",
    "        emb_matrix, vocab, unk_words = self.create_glove_emb_from_file(\n",
    "            self.glove_vectors_path, self.all_tokens, self.random_seed, self.emb_rand_uni_bound)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        knrm = KNRM(emb_matrix, freeze_embeddings=self.freeze_knrm_embeddings,\n",
    "                    out_layers=self.knrm_out_mlp, kernel_num=self.knrm_kernel_num)\n",
    "        return knrm, vocab, unk_words\n",
    "\n",
    "    def sample_data_for_train_iter(self, inp_df: pd.DataFrame, fill_top_to: int = 5,\n",
    "                                   min_group_size: int = 2, seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        tripl_df = inp_df_select.merge(inp_df_select.sample(frac=0.8), on='id_left')\n",
    "        tripl_df = tripl_df[tripl_df['id_right_x'] != tripl_df['id_right_y']]\n",
    "        tripl_df['label_1'] = (tripl_df['label_x'] - tripl_df['label_y']) > 0\n",
    "        # Смотрим сколько раз есть 1 в таргете\n",
    "        inf_df_group_sizes = tripl_df.groupby('id_left')['label_1'].sum()\n",
    "        del tripl_df['label_1']\n",
    "        # Берем индексы вопросов, которые больше порога\n",
    "        glue_train_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "        # Оставляем только вопросы, прошедшие порог и группируем\n",
    "        groups = tripl_df[tripl_df.id_left.isin(\n",
    "            glue_train_leftids_to_use)].groupby('id_left')\n",
    "\n",
    "        out_pairs = []\n",
    "\n",
    "        np.random.seed(2)\n",
    "\n",
    "\n",
    "        all_ids = set(tripl_df['id_right_x']).union(set(tripl_df['id_right_y'])).union(set(tripl_df['id_left']))\n",
    "\n",
    "        # Итерируемся по получившемуся датасету\n",
    "        # Имеем в виду, что первый документ не может быть нерелевантнее второго\n",
    "        # (label_x == 0 & label_y == 1 такого быть не должно)\n",
    "        for id_left, group in groups:\n",
    "            # id_left - запрос, id_right_x - первый кандидат, id_right_y - второй кандидат\n",
    "            # ID первого кандидата, являющегося более РЕЛЕВАНТНЫМ, чем второй\n",
    "            ones_filter = (group.label_x > 0) & (group.label_y == 0)\n",
    "            ones_ids_right = group[ones_filter].id_right_x.values  # label == 1\n",
    "            ones_ids_left = group[ones_filter].id_right_y.values  # label == 0\n",
    "\n",
    "            # ID первого кандидата, являющегося НЕРЕЛЕВАНТНЫМ, как и второй\n",
    "            zeros_filter = (group.label_x == 0) & (group.label_y == 0)\n",
    "            zeroes_ids_right = group[zeros_filter].id_right_x.values  # label == 0\n",
    "            zeroes_ids_left = group[zeros_filter].id_right_y.values  # label == 0\n",
    "\n",
    "            sum_len = len(ones_ids_right) + len(zeroes_ids_right)\n",
    "            # Считаем сколько не достает до максимального числа примеров\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                # Рандомно выбираем НЕРЕЛЕВАНТНЫХ из общего множества ID, которые не встречаются в этом примере\n",
    "                cur_chosen = set(ones_ids_right).union(set(ones_ids_left)).union({id_left})\n",
    "                pad_sample = np.random.choice(\n",
    "                    list(all_ids - cur_chosen), num_pad_items * 2, replace=False).tolist()\n",
    "                pad_sample_right = [i for n, i in enumerate(pad_sample) if n % 2 == 0]\n",
    "                pad_sample_left = [i for n, i in enumerate(pad_sample) if n % 2 == 1]\n",
    "            else:\n",
    "                pad_sample_right = []\n",
    "                pad_sample_left = []\n",
    "            # Формируем итоговые список\n",
    "            # 1 - дубликат; 0.5 - похож но не дубликат, 0.5 - вообще мимо (pad_sample)\n",
    "            for i, j in zip(ones_ids_right, ones_ids_left):\n",
    "                out_pairs.append([id_left, i, j, 1])\n",
    "            for i, j in zip(zeroes_ids_right, zeroes_ids_left):\n",
    "                out_pairs.append([id_left, i, j, 0.5])\n",
    "            for i, j in zip(pad_sample_right, pad_sample_left):\n",
    "                out_pairs.append([id_left, i, j, 0.5])\n",
    "        return out_pairs\n",
    "\n",
    "    def create_val_pairs(self, inp_df: pd.DataFrame, fill_top_to: int = 15,\n",
    "                         min_group_size: int = 2, seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "        # Берем только нужные столбцы\n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        # Смотрим сколько раз встречается левый вопрос\n",
    "        inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "        # Берем индексы вопросов, которые больше порога\n",
    "        glue_dev_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "        # Оставляем только вопросы, прошедшие порог и группируем\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_dev_leftids_to_use)].groupby('id_left')\n",
    "\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "\n",
    "        out_pairs = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Итерируемся по получившемуся датасету\n",
    "        for id_left, group in groups:\n",
    "            # ID ПРАВОГО запроса, который является дубликатом ЛЕВОГО\n",
    "            ones_ids = group[group.label > 0].id_right.values\n",
    "            # ID ПРАВОГО запроса, который НЕ является дубликатом ЛЕВОГО\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            # Считаем сколько не достает до максимального числа примеров\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                # Рандомно выбираем из общего множества ID, которые не встречаются в этом примере\n",
    "                cur_chosen = set(ones_ids).union(\n",
    "                    set(zeroes_ids)).union({id_left})\n",
    "                pad_sample = np.random.choice(\n",
    "                    list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            # Формируем итоговые список\n",
    "            # 2 - дубликат; 1 - похож но не дубликат, 2 - вообще мимо\n",
    "            for i in ones_ids:\n",
    "                out_pairs.append([id_left, i, 2])\n",
    "            for i in zeroes_ids:\n",
    "                out_pairs.append([id_left, i, 1])\n",
    "            for i in pad_sample:\n",
    "                out_pairs.append([id_left, i, 0])\n",
    "        return out_pairs\n",
    "\n",
    "    def get_idx_to_text_mapping(self, inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "        left_dict = (\n",
    "            inp_df\n",
    "            [['id_left', 'text_left']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_left')\n",
    "            ['text_left']\n",
    "            .to_dict()\n",
    "        )\n",
    "        right_dict = (\n",
    "            inp_df\n",
    "            [['id_right', 'text_right']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_right')\n",
    "            ['text_right']\n",
    "            .to_dict()\n",
    "        )\n",
    "        left_dict.update(right_dict)\n",
    "        return left_dict\n",
    "\n",
    "    def ndcg_k(self, ys_true: np.array, ys_pred: np.array, ndcg_top_k: int = 10) -> float:\n",
    "        ideal_dcg = self.dcg(ys_true, ys_true, ndcg_top_k)\n",
    "        pred_dcg = self.dcg(ys_true, ys_pred, ndcg_top_k)\n",
    "        return (pred_dcg / ideal_dcg).item() if ideal_dcg != 0 else 0\n",
    "\n",
    "    def dcg(self, ys_true: np.array, ys_pred: np.array, ndcg_top_k: int):\n",
    "        argsort = np.argsort(ys_pred, axis=0)[::-1]\n",
    "        ys_true_sorted = ys_true[argsort]\n",
    "        ret = 0\n",
    "        for i, l in enumerate(ys_true_sorted[:ndcg_top_k], 1):\n",
    "            ret += (2 ** l - 1) / math.log2(1 + i)\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def valid(self, model: torch.nn.Module, val_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        with torch.no_grad():\n",
    "            labels_and_groups = val_dataloader.dataset.index_pairs_or_triplets\n",
    "            labels_and_groups = pd.DataFrame(labels_and_groups, columns=['left_id', 'right_id', 'rel'])\n",
    "\n",
    "            all_preds = []\n",
    "            for batch in (val_dataloader):\n",
    "                inp_1, y = batch\n",
    "                preds = model.predict(inp_1)\n",
    "                preds_np = preds.detach().numpy()\n",
    "                all_preds.append(preds_np)\n",
    "            all_preds = np.concatenate(all_preds, axis=0)\n",
    "            labels_and_groups['preds'] = all_preds\n",
    "\n",
    "            ndcgs = []\n",
    "            for cur_id in labels_and_groups.left_id.unique():\n",
    "                cur_df = labels_and_groups[labels_and_groups.left_id == cur_id]\n",
    "                ndcg = self.ndcg_k(cur_df.rel.values.reshape(-1), cur_df.preds.values.reshape(-1))\n",
    "                if np.isnan(ndcg):\n",
    "                    ndcgs.append(0)\n",
    "                else:\n",
    "                    ndcgs.append(ndcg)\n",
    "            return np.mean(ndcgs)\n",
    "\n",
    "    def train(self, n_epochs: int):\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=self.train_lr)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        print(f'Baseline NDCG {self.valid(self.model, self.val_dataloader)}')\n",
    "        for n_epoch in range(n_epochs):\n",
    "            # self.model.train()\n",
    "            if n_epoch % self.change_train_loader_ep == 0:\n",
    "                self.train_pairs_for_ndcg = self.sample_data_for_train_iter(self.glue_train_df)\n",
    "                self.train_dataset = TrainTripletsDataset(self.train_pairs_for_ndcg,\n",
    "                                                    self.idx_to_text_mapping_train,\n",
    "                                                    vocab=self.vocab,\n",
    "                                                    oov_val=self.vocab['OOV'],\n",
    "                                                    preproc_func=self.simple_preproc\n",
    "                                                    )\n",
    "                self.train_dataloader = torch.utils.data.DataLoader(\n",
    "                    self.train_dataset,\n",
    "                    batch_size=self.dataloader_bs,\n",
    "                    num_workers=0,\n",
    "                    collate_fn=collate_fn, shuffle=True\n",
    "                    )\n",
    "            for batch in self.train_dataloader:\n",
    "                # assert batch[2].unique().size()[0] == 2\n",
    "                preds = self.model(batch[0], batch[1])\n",
    "                loss = criterion(preds, batch[2])\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                print(f'Loss {loss.item()}')\n",
    "            print(f'Epoch {n_epoch + 1}: mean NDCG {self.valid(self.model, self.val_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwSGfFRTr6CT",
    "outputId": "9687028d-0c08-4512-e2aa-2a9e9b0f8bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.get_glue_df('train') 3\n",
      "self.get_glue_df('dev') 1\n",
      "self.create_val_pairs 107\n",
      "self.get_all_tokens 0\n",
      "glove_emb 400000\n",
      "inner_keys 400000\n",
      "self.build_knrm_model 83\n",
      "self.get_idx_to_text_mapping 1\n",
      "self.idx_to_text_mapping_dev 0\n",
      "CPU times: user 3min 7s, sys: 2.54 s, total: 3min 10s\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s = Solution(glue_qqp_dir = '/content/QQP', glove_vectors_path = '/content/glove.6B.50d.txt', knrm_out_mlp=[], freeze_knrm_embeddings=True, train_lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0OzSRRq4P9o",
    "outputId": "d61821d0-b306-42be-b5cc-e2ab34fe1ae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline NDCG 0.411420366205633\n",
      "Loss 1.041214942932129\n",
      "Loss 1.1276235580444336\n",
      "Loss 0.7976537942886353\n",
      "Loss 0.784345805644989\n",
      "Loss 0.7953973412513733\n",
      "Loss 0.8058797717094421\n",
      "Loss 0.7783726453781128\n",
      "Loss 0.756111741065979\n",
      "Loss 0.7875634431838989\n",
      "Loss 0.77696692943573\n",
      "Loss 0.7584382891654968\n",
      "Loss 0.7275822758674622\n",
      "Loss 0.7607614398002625\n",
      "Loss 0.7257606387138367\n",
      "Loss 0.7532147169113159\n",
      "Loss 0.7221930623054504\n",
      "Loss 0.7237700819969177\n",
      "Loss 0.7482706904411316\n",
      "Loss 0.7351915836334229\n",
      "Loss 0.7341530323028564\n",
      "Loss 0.7251180410385132\n",
      "Loss 0.724047064781189\n",
      "Loss 0.7236614227294922\n",
      "Epoch 1: mean NDCG 0.5260306691098524\n",
      "Loss 0.7088751196861267\n",
      "Loss 0.7389498949050903\n",
      "Loss 0.7080919146537781\n",
      "Loss 0.7210667729377747\n",
      "Loss 0.731951892375946\n",
      "Loss 0.7185767889022827\n",
      "Loss 0.7334063053131104\n",
      "Loss 0.7173170447349548\n",
      "Loss 0.7064782381057739\n",
      "Loss 0.7313122153282166\n",
      "Loss 0.7249554395675659\n",
      "Loss 0.7044662237167358\n",
      "Loss 0.7269113659858704\n",
      "Loss 0.7142452001571655\n",
      "Loss 0.8230769038200378\n",
      "Loss 0.7308399081230164\n",
      "Loss 1.037440538406372\n",
      "Loss 0.7910645604133606\n",
      "Loss 1.119854211807251\n",
      "Loss 1.2483659982681274\n",
      "Loss 0.6961446404457092\n",
      "Loss 0.7128503918647766\n",
      "Loss 0.7167330980300903\n",
      "Epoch 2: mean NDCG 0.5960804684013176\n",
      "Loss 0.7003134489059448\n",
      "Loss 0.7047958374023438\n",
      "Loss 0.7169051170349121\n",
      "Loss 0.6953112483024597\n",
      "Loss 0.6934491395950317\n",
      "Loss 0.7057158350944519\n",
      "Loss 0.6925127506256104\n",
      "Loss 0.7349627017974854\n",
      "Loss 0.6970032453536987\n",
      "Loss 0.7109538912773132\n",
      "Loss 0.8316746950149536\n",
      "Loss 1.7994580268859863\n",
      "Loss 0.7290436029434204\n",
      "Loss 0.7380709052085876\n",
      "Loss 0.9798647165298462\n",
      "Loss 0.691576361656189\n",
      "Loss 0.6973637342453003\n",
      "Loss 0.6844260096549988\n",
      "Loss 0.7151573300361633\n",
      "Loss 0.7048928737640381\n",
      "Loss 0.7035780549049377\n",
      "Loss 0.6912675499916077\n",
      "Loss 0.6700320243835449\n",
      "Epoch 3: mean NDCG 0.7427303359622963\n",
      "Loss 0.7043060064315796\n",
      "Loss 0.6989958882331848\n",
      "Loss 0.6868589520454407\n",
      "Loss 0.6958927512168884\n",
      "Loss 0.7119406461715698\n",
      "Loss 0.6873775720596313\n",
      "Loss 0.6957187056541443\n",
      "Loss 0.6914100050926208\n",
      "Loss 0.6901973485946655\n",
      "Loss 0.6840280294418335\n",
      "Loss 0.6924927830696106\n",
      "Loss 0.6804369688034058\n",
      "Loss 0.6813288927078247\n",
      "Loss 0.6880934238433838\n",
      "Loss 0.6847935318946838\n",
      "Loss 0.6841525435447693\n",
      "Loss 0.6854937672615051\n",
      "Loss 0.7045680284500122\n",
      "Loss 0.7047123908996582\n",
      "Loss 0.6852179765701294\n",
      "Loss 0.7072708606719971\n",
      "Loss 0.689466655254364\n",
      "Loss 0.6864911317825317\n",
      "Epoch 4: mean NDCG 0.8211886574521662\n",
      "Loss 0.6884806156158447\n",
      "Loss 0.6930204629898071\n",
      "Loss 0.686913013458252\n",
      "Loss 0.6776570677757263\n",
      "Loss 0.678013801574707\n",
      "Loss 0.6830621957778931\n",
      "Loss 0.6987280249595642\n",
      "Loss 0.6773032546043396\n",
      "Loss 0.6828228235244751\n",
      "Loss 0.6967653036117554\n",
      "Loss 0.6879914999008179\n",
      "Loss 0.6778497695922852\n",
      "Loss 0.6907579898834229\n",
      "Loss 0.6933861970901489\n",
      "Loss 0.7030006647109985\n",
      "Loss 0.6875193119049072\n",
      "Loss 0.6799420118331909\n",
      "Loss 0.6927376985549927\n",
      "Loss 0.6858367919921875\n",
      "Loss 0.6867566704750061\n",
      "Loss 0.67739337682724\n",
      "Loss 0.6848819255828857\n",
      "Loss 0.6801952123641968\n",
      "Epoch 5: mean NDCG 0.8847387959645634\n",
      "Loss 0.6812101006507874\n",
      "Loss 0.6910663843154907\n",
      "Loss 0.6800056099891663\n",
      "Loss 0.6835224628448486\n",
      "Loss 0.6775602698326111\n",
      "Loss 0.693395733833313\n",
      "Loss 0.6928085684776306\n",
      "Loss 0.6751799583435059\n",
      "Loss 0.6763821244239807\n",
      "Loss 0.682014524936676\n",
      "Loss 0.673617422580719\n",
      "Loss 0.6748049259185791\n",
      "Loss 0.687069833278656\n",
      "Loss 0.6873520612716675\n",
      "Loss 0.679282009601593\n",
      "Loss 0.692233681678772\n",
      "Loss 0.6867402791976929\n",
      "Loss 0.6747463345527649\n",
      "Loss 0.6825979948043823\n",
      "Loss 0.6747548580169678\n",
      "Loss 0.688628077507019\n",
      "Loss 0.6715306043624878\n",
      "Loss 0.677966833114624\n",
      "Epoch 6: mean NDCG 0.8782754374175039\n",
      "Loss 0.6790477633476257\n",
      "Loss 0.69819176197052\n",
      "Loss 0.6816669702529907\n",
      "Loss 0.6799570322036743\n",
      "Loss 0.6752762794494629\n",
      "Loss 0.6794794201850891\n",
      "Loss 0.6634278297424316\n",
      "Loss 0.6704283952713013\n",
      "Loss 0.6770144104957581\n",
      "Loss 0.6826721429824829\n",
      "Loss 0.696463942527771\n",
      "Loss 0.6856158971786499\n",
      "Loss 0.6851685643196106\n",
      "Loss 0.6754525303840637\n",
      "Loss 0.6810144782066345\n",
      "Loss 0.6858046054840088\n",
      "Loss 0.6790761947631836\n",
      "Loss 0.6790093779563904\n",
      "Loss 0.6769853234291077\n",
      "Loss 0.6749703288078308\n",
      "Loss 0.6751964092254639\n",
      "Loss 0.681179940700531\n",
      "Loss 0.6890068650245667\n",
      "Epoch 7: mean NDCG 0.8955070193899947\n",
      "Loss 0.6846712827682495\n",
      "Loss 0.6790452003479004\n",
      "Loss 0.678457498550415\n",
      "Loss 0.6868649125099182\n",
      "Loss 0.6786799430847168\n",
      "Loss 0.6761918663978577\n",
      "Loss 0.6804401874542236\n",
      "Loss 0.6770401000976562\n",
      "Loss 0.6825217604637146\n",
      "Loss 0.6810405254364014\n",
      "Loss 0.6859123110771179\n",
      "Loss 0.6771950721740723\n",
      "Loss 0.6807416081428528\n",
      "Loss 0.677244246006012\n",
      "Loss 0.6661570072174072\n",
      "Loss 0.6823399662971497\n",
      "Loss 0.6746343970298767\n",
      "Loss 0.6759941577911377\n",
      "Loss 0.6919686794281006\n",
      "Loss 0.6939894556999207\n",
      "Loss 0.68600994348526\n",
      "Loss 0.6760552525520325\n",
      "Loss 0.6723303198814392\n",
      "Epoch 8: mean NDCG 0.9020223171334165\n",
      "Loss 0.6737240552902222\n",
      "Loss 0.6807649731636047\n",
      "Loss 0.6805310249328613\n",
      "Loss 0.6698616743087769\n",
      "Loss 0.6859042048454285\n",
      "Loss 0.691316545009613\n",
      "Loss 0.6872283816337585\n",
      "Loss 0.677682101726532\n",
      "Loss 0.6838983297348022\n",
      "Loss 0.6641432642936707\n",
      "Loss 0.6775282621383667\n",
      "Loss 0.6829050183296204\n",
      "Loss 0.6855170130729675\n",
      "Loss 0.6680593490600586\n",
      "Loss 0.672492504119873\n",
      "Loss 0.6775466203689575\n",
      "Loss 0.6824482083320618\n",
      "Loss 0.6843221187591553\n",
      "Loss 0.676177442073822\n",
      "Loss 0.6877678632736206\n",
      "Loss 0.6838773488998413\n",
      "Loss 0.6950371265411377\n",
      "Loss 0.6776955127716064\n",
      "Epoch 9: mean NDCG 0.9141090375694736\n",
      "CPU times: user 12min 20s, sys: 2min 49s, total: 15min 9s\n",
      "Wall time: 15min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s.train(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UI9CrNaXlx8T"
   },
   "outputs": [],
   "source": [
    "torch.save(s.model.embeddings.state_dict(), 'embeddings.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sYnv74361Hx"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('vocab.json', \"w\") as file:\n",
    "    json.dump(s.vocab, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2y4n4gwC-bnI"
   },
   "outputs": [],
   "source": [
    " torch.save(s.model.mlp.state_dict(), 'knrm_mlp.bin')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
